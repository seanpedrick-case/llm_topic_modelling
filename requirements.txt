pandas==2.3.0
gradio==5.36.2
transformers==4.53.2
spaces==0.37.0
boto3==1.39.4
pyarrow==20.0.0
openpyxl==3.1.5
markdown==3.7
tabulate==0.9.0
lxml==5.3.0
google-genai==1.21.1
html5lib==1.1
beautifulsoup4==4.12.3
rapidfuzz==3.13.0
torch==2.6.0 --extra-index-url https://download.pytorch.org/whl/cu124
# CPU only: torch==2.7.1 --extra-index-url https://download.pytorch.org/whl/cpu
https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu124/llama_cpp_python-0.3.4-cp310-cp310-linux_x86_64.whl # Specify exact llama_cpp wheel for huggingface compatibility. Will not work with Gemma 3, only Gemma 2
# llama-cpp-python==0.3.9 - will work on Hugging Face spaces, but will not work for Zero GPU or GPU spaces, only CPU. Can then use Gemma 3.
# llama-cpp-python==0.3.9 -C cmake.args="-DGGML_CUDA=on" # CUDA version. Will not work on Hugging Face spaces as NVCC is not installed in their spaces
#llama-cpp-python==0.3.9 -C cmake.args="-DGGML_BLAS=ON;-DGGML_BLAS_VENDOR=OpenBLAS"# Linux compatibility - for recent models like Gemma 3 (Not Hugging Face)
# For Windows try the following
# llama-cpp-python==0.3.9 -C cmake.args="-DGGML_BLAS=ON;-DGGML_BLAS_VENDOR=OpenBLAS;-DBLAS_INCLUDE_DIRS=C:/<root-path-to-openblas>/OpenBLAS/include;-DBLAS_LIBRARIES=C:/<root-path-to-openblas>/OpenBLAS/lib/libopenblas.lib
#https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.2/llama_cpp_python-0.3.2-cp311-cp311-win_amd64.whl # Use this for Windows if abov doesn't work, enough for Gemma 2b
#llama-cpp-python==0.3.2 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu # Use this for guaranteed Linux compatibility - enough for Gemma 2b only
python-dotenv==1.1.0
#numpy==1.26.4
#typing_extensions==4.12.2
