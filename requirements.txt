pandas==2.2.3
gradio==5.34.2
spaces==0.37.0
boto3==1.38.38
pyarrow==19.0.1
openpyxl==3.1.3
markdown==3.7
tabulate==0.9.0
lxml==5.3.0
google-generativeai==0.8.4
html5lib==1.1
beautifulsoup4==4.12.3
rapidfuzz==3.10.1
torch==2.7.1 --extra-index-url https://download.pytorch.org/whl/cu126
# CPU only: torch==2.7.1 --extra-index-url https://download.pytorch.org/whl/cpu
llama-cpp-python==0.3.9 -C cmake.args="-DGGML_CUDA=on" # CUDA version by default
#llama-cpp-python==0.3.9 -C cmake.args="-DGGML_BLAS=ON;-DGGML_BLAS_VENDOR=OpenBLAS"# Linux compatibility - for recent models like Gemma 3
# For Windows try the following
# llama-cpp-python==0.3.9 -C cmake.args="-DGGML_BLAS=ON;-DGGML_BLAS_VENDOR=OpenBLAS;-DBLAS_INCLUDE_DIRS=C:/<root-path-to-openblas>/OpenBLAS/include;-DBLAS_LIBRARIES=C:/<root-path-to-openblas>/OpenBLAS/lib/libopenblas.lib
#https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.2/llama_cpp_python-0.3.2-cp311-cp311-win_amd64.whl # Use this for Windows if abov doesn't work, enough for Gemma 2b
#llama-cpp-python==0.3.2 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu # Use this for guaranteed Linux compatibility - enough for Gemma 2b only
transformers==4.51.1
python-dotenv==1.1.0
#numpy==1.26.4
typing_extensions==4.12.2
